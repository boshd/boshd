---
title: "Gradient descent and beyond"
category: "note"
excerpt: "some explanation here"
# coverImage: './public/posts/figure1.png'
date: "2023-02-20T05:35:07.322Z"
author:
  name: Kareem
  # picture: '/assets/blog/authors/jj.jpeg'
ogImage:
  url: "/assets/blog/dynamic-routing/cover.jpg"
---

<!-- We'll start by defining each concept and use them to build up to gradient descent. -->

In machine learning, optimization is king. The ability to find the best possible values for a model's parameters is crucial for creating accurate models that can yield useful predictions. At the heart of a machine learning algorithm lies a powerful optimization technique known as **gradient descent** that iteratively tweaks a model's parameters in the direction of steepset descent which minimizes it's loss function. Beyond this foundational method lies a wealth of extensions, refinements, and alternatives that have been developed over the years to tackle the challenges of modern machine learning. In this article, I'll give a gentle introduction of several of these methods and briefly dive into their mathematical frameworks.

<!-- ## Jacobian

The Jacobian is a matrix of first-order partial derivatives of a multivariate function. Often, the Jacobian is used in gradient descent  -->

<!-- The Jacobian matrix collects all first-order partial derivatives of a multivariate function that can be used for backpropagation.
The Jacobian determinant is useful in changing between variables, where it acts as a scaling factor between one coordinate space and another. -->



<!-- $$
\mathbb{J}=\left[\begin{array}{ccc}
\dfrac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{1}} & \cdots & \dfrac{\partial \mathbf{f}(\mathbf{x})}{\partial x_{n}}
\end{array}\right]=\left[\begin{array}{c}
\nabla^{T} f_{1}(\mathbf{x}) \\
\vdots \\
\nabla^{T} f_{m}(\mathbf{x})
\end{array}\right]=\left[\begin{array}{ccc}
\dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{1}} & \cdots & \dfrac{\partial f_{1}(\mathbf{x})}{\partial x_{n}} \\
\vdots & \ddots & \vdots \\
\dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{1}} & \cdots & \dfrac{\partial f_{m}(\mathbf{x})}{\partial x_{n}}
\end{array}\right]
$$ -->

## Vanilla gradient descent

Gradient descent is an algorithm that minimizes a given model's loss function based on calculating the gradients of the loss landscape. Believe it or not, Gradient Descent was introduced by mathemetician and physicist Louis Augustin Cauchy in *1847*.

We start with a loss/error function $L_\varepsilon\$ that quantifies the difference between a model's predictions and the true values. The goal is then to tweak the model's parameters/weights so until the error is minimized. The model starts out with randombly generated weights and iteratively changes them until they somewhat resemble the function we're trying to estimate.

Tweaking the parameters is done by calculating the gradient of the loss function at a specific point which determines which direction we need to take to minimize the loss. This is given by

$$
x_{n+1} = x_{n} - \epsilon \nabla f(x_{n})
$$

where $\epsilon$ is the learning rate or step size.

#### **Gradient**

The gradient is a vector of first order derivatives of a scalar-valued funciton. It is given by:

$$
\nabla f(x,y, ..., z) = \begin{bmatrix}
                    \frac{\partial f}{\partial x} \\
                    \frac{\partial f}{\partial y} \\
                    \vdots \\
                    \frac{\partial f}{\partial z}
                  \end{bmatrix}
$$

The gradient helps us measure the direction of the fastest rate of increase. Imagine you're at the peak of a mountain range, looking down; what is the fastest path to the bottom? That is what the gradient tells us for any function in $R^{n}$.

Let's begin with an example. How do you find the minimum of the following function?

$$
f(x) = x^{2}
$$

Let's first claculate the gradient of $f(x)$ w.r.t $x$ which is simply the derivative:

$$
\frac{\partial f}{\partial x} = 2x
$$


Then lets set $\epsilon=0.1$ and pick a random point $x_{0}=4$ to begin with. We can then take steps in the direction of the negative gradient of the function as follows, using the gradient descent equation we defined above:

$$
x_{1} = 3 - (0.1 * 2(3)) = 2.4
$$
$$
x_{2} = 2.4 - (0.1 * 2(2.4)) = 1.92
$$
$$
x_{3} = 1.92 - (0.1 * 2(1.92)) = 1.536
$$
<!-- $$
x_{4} = 1.536 - (0.1 * 2(1.536)) = 1.2288
$$ -->
$$****
\vdots
$$

![gradient descent on parabola](/posts/hessian/5figs.png "Gradient descent on parabola")

As we can see in the figures above, picking a suitable value for $\epsilon$ is very important to ensure that our algorithm behaves correctly. Picking a value for $\epsilon$ that is too small can result in us undershooting the minimum and taking a long time to arrive there ($\epsilon=0.2$ figure). On the other hand, if we pick a value for $\epsilon$ that is too large, we risk overshooting the minimum as shown in the $\**epsilon**=0.8$ figure.

And that's it! We calculated the gradient of $x^2$, defined our decsent method $x_{n+1} = x_{n} - \epsilon \nabla f(x_{n})$, picked a suitable value for our learning rate $\epsilon$, a random starting point $x_0$ and successfully found the minimum.

In practice, we are often interested **in** finding the minimum of much mroe complicated functions with multiple variables $f(x, y, z, ..., k)$.

## **Gradient descent variants**

#### **Momentum gradient descent**

#### **Nesterov Accelerated Gradient Descent**

## **Gradient descent alternatives**

### ****

#### **Batch gradient descent**

#### **Taylor approximation**

The Taylor approximation of the function $f(x)$ around the point $x^{(0)}$ can be used to help us choose a suitable step size $\epsilon$ that minimizes the function $f(x)$:

Here, $||\nabla f(x^{(0)})||^2$ is the magnitude of the gradient of $f$ evaluated at $x^{(0)}$.

In theory, the taylor approximation is infinite, hence in our case we will incur some error because we cut it off at the second derivative, (see eq. 1), using a learning rate $\epsilon$, the new point $x$ is given by $x_0 - \epsilon g$. Subbing this we get,

$$
x = x_0 - \epsilon g
$$

The new point $x$ is the old $x_0$ minus a small step in the direction of the gradient which dictates the signage of the expression $x_0 - \epsilon g$.

Now we have,

$$
f(x_0 - \epsilon g) \approx f(x_0) - \epsilon g^Tg + \frac{1}{2} \epsilon^2 g^THg
$$

where,
$f(x_0)$ is the original value of the funciton,
$\epsilon g^Tg$ is expected imporvement due to the slope of the function, and
$\frac{1}{2} \epsilon^2 g^THg$ is the correction to account for function's curvature.

Note that when $\frac{1}{2} \epsilon^2 g^THg$ is large enough, the gradient descent can move uphill.

Finally, the different terms in the Taylor approximation are just tools to help us adjust to the different properties of the landsacape.

If the last term is positive, the optimal step that secreases the Taylor approximation the most yields:

$$
\epsilon^* = \frac{g^Tg}{g^THg}
$$

We can use this optimal step size $\epsilon^*$ to update our current point $x_0$ as follows:

$$
x = x_0 - \epsilon g
$$

This update rule is known as the Newton-Raphson method, and it can converge much faster than gradient descent since it takes into account the curvature of the landscape. However, the computation of the Hessian matrix can be expensive for high-dimensional functions, and it may not always be positive definite, which is a requirement for the method to work properly.

In practice, gradient descent is often a more practical optimization algorithm, as it is simpler to implement and can work well for many optimization problems. However, there are cases where Newton-Raphson method or its variants, such as quasi-Newton methods like Broyden-Fletcher-Goldfarb-Shanno (BFGS) and limited-memory BFGS (L-BFGS), can outperform gradient descent.

In summary, the Hessian matrix is a powerful tool for understanding the curvature of a multivariate function and can be used to improve optimization algorithms such as gradient descent. However, its computation can be expensive, and other optimization methods may be more practical for some problems.

## Newton's method and the Hessian

The Hessian matrix is a matrix of the second derivatives of a multivariate function. It describes the curvature of the landscape produced by that function. In other words, the Hessian matrix tells us how much the gradient of a function changes as we move along each coordinate axis.

This is useful when we want to not only know which direction

We can define the Hessian matrix of a function $f(x)$ as follows:

$$
H(f)(x)_{i,j} = \frac{\partial^2}{\partial x_i \partial x_j} f(x)
$$

Here, $H(f)(x)$ is the Hessian matrix of the function $f$ evaluated at point $x$, and $H_{i,j}(f)(x)$ is its $(i,j)$-th entry.

## Taylor Approximation

The Hessian matrix can be used to create a second-order Taylor approximation of a function $f(x)$ around the current point $x^{(0)}$:

$$
f(x) \approx f(x_0) + (x-x_0)^T \triangledown f + \frac{1}{2} (x-x_0)^T H(x-x_0)
$$

Here, $\nabla f(x^{(0)})$ is the gradient of the function $f$ evaluated at point $x^{(0)}$.

The Taylor approximation allows us to approximate the function $f(x)$ by a quadratic function of the form $f(x^{(0)}) + \nabla f(x^{(0)})^T (x-x^{(0)}) + \frac{1}{2} (x-x^{(0)})^T H(f)(x^{(0)})(x-x^{(0)})$. This approximation is useful because it captures the local behavior of the function around the point $x^{(0)}$.